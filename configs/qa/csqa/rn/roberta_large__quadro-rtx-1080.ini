[DEFAULT]
dataset : csqa
split_type : inhouse

task : qa
arch : roberta_large

text_encoder_head : bos_token_mlp
text_in_dim : 1024
text_out_dim : 1024
text_hidden_dim : 1024
text_hidden_layers : 1
text_layer_norm : True
text_dropout : False

graph_encoder : rn
graph_init_range : 0.02
graph_emb_scale : 1.0
graph_freeze_ent_emb : True
graph_mlp_dim : 128
graph_mlp_layer_num : 2
graph_dropoutm : 0.3
graph_fc_dim : 128
graph_fc_layer_num : 0
graph_pool : multihead_pool
graph_att_head_num : 2

ent_emb_dim : 1024
rel_emb_dim : 100

saliency_mode : none
saliency_source : none
saliency_method : none
save_saliency : False

max_epochs : 100
train_batch_size : 1
eval_batch_size : 1
accumulate_grad_batches : 1

text_lr : 1e-5
graph_lr : 3e-4
optimizer : radam
lr_scheduler : fixed
warmup_updates : 1000

num_workers : 0
gpus : 1

seed : 0
fp16 : True