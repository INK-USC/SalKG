[DEFAULT]
dataset : csqa
split_type : inhouse

task : saliency
arch : roberta_large

text_encoder_head : bos_token_mlp
text_in_dim : 1024
text_out_dim : 1024
text_hidden_dim : 1024
text_hidden_layers : 1
text_layer_norm : True
text_dropout : False

graph_encoder : mhgrn
sal_gnn_layer_type : gine
sal_gnn_io_dim : 256
sal_gnn_hidden_dim : 256
sal_gnn_hidden_layers : 1
sal_gnn_layer_norm : True
sal_gnn_dropout : False
sal_gnn_layers : 1
sal_cls_hidden_dim : 1024
sal_cls_hidden_layers : 1
sal_cls_layer_norm : True
sal_cls_dropout : False
pos_weight : 1

ent_emb_dim : 1024
rel_emb_dim : 100

saliency_mode : fine
saliency_source : target
saliency_method : grad
save_saliency : False
top_k : 10

max_epochs : 100
train_batch_size : 1
eval_batch_size : 1
accumulate_grad_batches : 1

text_lr : 1e-5
graph_lr : 1e-4
optimizer : radam
lr_scheduler : fixed
warmup_updates : 1000

num_workers : 10
gpus : 1

seed : 0
fp16 : False
