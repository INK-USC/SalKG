[DEFAULT]
dataset : csqa
split_type : inhouse

task : saliency
arch : roberta_large
num_choices : 5

text_encoder_head : bos_token_mlp
text_in_dim : 1024
text_out_dim : 1024
text_hidden_dim : 1024
text_hidden_layers : 1
text_layer_norm : True
text_dropout : False

graph_encoder : mhgrn
sal_gnn_layer_type : gine
sal_gnn_io_dim : 256
sal_gnn_hidden_dim : 256
sal_gnn_hidden_layers : 1
sal_gnn_layer_norm : True
sal_gnn_dropout : False
sal_gnn_layers : 1
sal_cls_hidden_dim : 1024
sal_cls_hidden_layers : 1
sal_cls_layer_norm : True
sal_cls_dropout : False
pos_weight : 1

ent_emb_dim : 1024
rel_emb_dim : 100

saliency_mode : coarse
saliency_source : target
saliency_method : occl
save_saliency : True
ckpt_path : /home/aaron/FactSelection/save/FS-989/checkpoints/epoch=8-step=2393.ckpt

max_epochs : 100
train_batch_size : 1
eval_batch_size : 1
accumulate_grad_batches : 1

text_lr : 1e-5
graph_lr : 1e-3
optimizer : radam
lr_scheduler : fixed
warmup_updates : 1000

num_workers : 10
gpus : 1

seed : 0
fp16 : False
